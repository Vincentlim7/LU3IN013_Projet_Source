{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the UniRep mLSTM \"babbler\". This version demonstrates the 64-unit and the 1900-unit architecture. \n",
    "\n",
    "We recommend getting started with the 64-unit architecture as it is easier and faster to run, but has the same interface as the 1900-unit one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the 64-unit or the 1900-unit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "USE_FULL_1900_DIM_MODEL = False # if True use 1900 dimensional model, else use 64 dimensional one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "if USE_FULL_1900_DIM_MODEL:\n",
    "    # Sync relevant weight files\n",
    "    !aws s3 sync --no-sign-request --quiet s3://unirep-public/1900_weights/ 1900_weights/\n",
    "    \n",
    "    # Import the mLSTM babbler model\n",
    "    from unirep import babbler1900 as babbler\n",
    "    \n",
    "    # Where model weights are stored.\n",
    "    MODEL_WEIGHT_PATH = \"./1900_weights\"\n",
    "    \n",
    "else:\n",
    "    # Sync relevant weight files\n",
    "    !aws s3 sync --no-sign-request --quiet s3://unirep-public/64_weights/ 64_weights/\n",
    "    \n",
    "    # Import the mLSTM babbler model\n",
    "    from unirep import babbler64 as babbler\n",
    "    \n",
    "    # Where model weights are stored.\n",
    "    MODEL_WEIGHT_PATH = \"./64_weights\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data formatting and management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize UniRep, also referred to as the \"babbler\" in our code. You need to provide the batch size you will use and the path to the weight directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "b = babbler(batch_size=batch_size, model_path=MODEL_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prot_seq(file_name, cpt):\n",
    "    if cpt % 30 == 0:\n",
    "        print(\"protein nÂ°\", cpt)\n",
    "        lecture_start_time = time.time()\n",
    "        f = open(\"dataset/fastas/\" + file_name + \".fasta\", \"r\") # Retriving the file containing the sequence\n",
    "        next(f) # Skipping the first line (containing the protein's name)\n",
    "        seq = \"\"\n",
    "        for line in f: # Retriving the sequence\n",
    "            tmp = line.rstrip()    # Deleting \"\\n\"\n",
    "            seq += tmp\n",
    "        f.close\n",
    "        lecture_elapsed_time = time.time() - lecture_start_time\n",
    "        print(\"Temps lecture fichier : \", lecture_elapsed_time)\n",
    "    else:\n",
    "        f = open(\"dataset/fastas/\" + file_name + \".fasta\", \"r\") # Retriving the file containing the sequence\n",
    "        next(f) # Skipping the first line (containing the protein's name)\n",
    "        seq = \"\"\n",
    "        for line in f: # Retriving the sequence\n",
    "            tmp = line.rstrip()    # Deleting \"\\n\"\n",
    "            seq += tmp\n",
    "        f.close\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def get_vecs(seq, cpt):\n",
    "    if cpt % 30 == 0:\n",
    "        vecs_start_time = time.time()\n",
    "        vecs = b.get_rep(seq)\n",
    "        avg_vec = vecs[0]\n",
    "        concat_vec = np.reshape(vecs, 192) # Concatenation of all three vectors\n",
    "        vecs_elapsed_time = time.time() - vecs_start_time\n",
    "        print(\"Temps vecs : \", vecs_elapsed_time)\n",
    "    \n",
    "    else:\n",
    "        vecs = b.get_rep(seq)\n",
    "        avg_vec = vecs[0]\n",
    "        concat_vec = np.reshape(vecs, 192) # Concatenation of all three vectors\n",
    "    return avg_vec, concat_vec\n",
    "\n",
    "def get_classe(searched_protein): # Returning the protein's category (the key in the level 0 dictionnary)\n",
    "    for classe, protein_list in classes.items(): # Browsing the category dictionnary (level 0)\n",
    "        for protein_name, seq in protein_list.items(): # Browsing the protein dictionnary (level 1)\n",
    "            if protein_name == searched_protein:\n",
    "                return classe\n",
    "\n",
    "\n",
    "def dic_init(): # Initializing nested dictionnaries all proteins and their vector (avg or concatenated)\n",
    "    classes_avg = dict()\n",
    "    classes_concat = dict()\n",
    "    f = open(\"partialProtein.list\", \"r\")\n",
    "    cpt = 0 # Number of protein already processed\n",
    "    start_time = time.time()\n",
    "    for line in f: # Browsing all protein\n",
    "        infos = line.split()\n",
    "        protein = infos[0]    # Protein name\n",
    "        classe = infos[1]     # Protein category\n",
    "        if classe not in classes_avg: # Adding new category key if it doesn't exist\n",
    "            classes_avg[classe] = dict()\n",
    "        if classe not in classes_concat: # Adding new category key if it doesn't exist\n",
    "            classes_concat[classe] = dict()\n",
    "        prot_seq = get_prot_seq(protein, cpt) # Retrieving protein sequence\n",
    "        classes_avg[classe][protein], classes_concat[classe][protein] = get_vecs(prot_seq, cpt) # Retrieving and stocking vectors\n",
    "        cpt += 1\n",
    "        if cpt % 100 == 0: # Periodical save every 100 protein processed\n",
    "            print(\"Nombre proteines lues : \", cpt)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(elapsed_time)\n",
    "            start_time = time.time() # Reset timer\n",
    "            np.save(\"database/next_batch/data_avg\" + str(cpt) + \".npy\", classes_avg)\n",
    "            np.save(\"database/next_batch/data_concat\" + str(cpt) + \".npy\", classes_concat)\n",
    "    np.save(\"database/next_batch/data_avg.npy\", classes_avg) # Saving whole database\n",
    "    np.save(\"database/next_batch/data_concat.npy\", classes_concat)\n",
    "    return classes_avg, classes_concat\n",
    "\n",
    "def get_dist_intra(protein_dict): # Initializing a dictionnary containning the shortest euclidian distance between proteins of the same category\n",
    "    dist_intra = dict()\n",
    "    for classe, protein_list in protein_dict.items():\n",
    "        if classe not in dist_intra: # Adding new category key if it doesn't exist\n",
    "            dist_intra[classe] = dict()\n",
    "        for protein_a, vec_a in protein_list.items():\n",
    "            dist_intra[classe][protein_a] = (None, np.inf)\n",
    "            for protein_b, vec_b in protein_list.items():\n",
    "                if protein_a == protein_b:\n",
    "                    continue\n",
    "                dist = distance.euclidean(vec_a, vec_b)\n",
    "                if dist < dist_intra[classe][protein_a][1]:\n",
    "                    dist_intra[classe][protein_a] = (protein_b, dist)\n",
    "    return dist_intra\n",
    "\n",
    "def get_dist_extra(protein_dict): # Initializing a dictionnary containning the shortest euclidian distance between proteins of different category\n",
    "    dist_extra = dict()\n",
    "    for classe_a, protein_list_a in protein_dict.items():\n",
    "        if classe_a not in dist_extra: # Adding new category key if it doesn't exist\n",
    "            dist_extra[classe_a] = dict()\n",
    "        for protein_a, vec_a in protein_list_a.items():\n",
    "            dist_extra[classe_a][protein_a] = (None, np.inf)\n",
    "            for classe_b, protein_list_b in protein_dict.items():\n",
    "                if classe_a == classe_b:\n",
    "                    continue\n",
    "                for protein_b, vec_b in protein_list_b.items():\n",
    "                    dist = distance.euclidean(vec_a, vec_b)\n",
    "                    if dist < dist_extra[classe_a][protein_a][1]:\n",
    "                        dist_extra[classe_a][protein_a] = (protein_b, dist)\n",
    "    return dist_extra\n",
    "                    \n",
    "def histo(dist_intra, dist_extra):\n",
    "    x_intra = []\n",
    "    y_intra = []\n",
    "    x_extra = []\n",
    "    y_extra = []\n",
    "\n",
    "    for classe, protein_list in dist_intra.items(): # Retrieving smallest dist value in dist_intra for each protein\n",
    "        classe_dist = np.inf\n",
    "        for protein, val in protein_list.items():\n",
    "            if(val[1] < classe_dist):\n",
    "                classe_dist = val[1]\n",
    "        if(classe_dist != np.inf):\n",
    "            x_intra.append(classe_dist)\n",
    "            y_intra.append(len(protein_list))\n",
    "\n",
    "    for classe, protein_list in dist_extra.items(): # Retrieving smallest dist value in dist_extra for each protein\n",
    "        classe_dist = np.inf\n",
    "        for protein, val in protein_list.items():\n",
    "            if(val[1] < classe_dist):\n",
    "                classe_dist = val[1]\n",
    "        if(classe_dist != np.inf):\n",
    "            x_extra.append(classe_dist)\n",
    "            y_extra.append(len(protein_list))\n",
    "\n",
    "    plt.bar(x_intra,y_intra,align='center', alpha = 0.7, width = 0.01, label='intra')\n",
    "    plt.bar(x_extra,y_extra,align='center', alpha = 0.7, width = 0.01, label='extra')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Nb Sequence')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_start_time = time.time()\n",
    "classes_avg, classes_concat = dic_init()\n",
    "total_elapsed_time = time.time() - total_start_time\n",
    "print(total_elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_avg = np.load(\"database/data_avg15114.npy\")[()]\n",
    "classes_concat = np.load(\"database/data_concat15114.npy\")[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_intra_avg = get_dist_intra(classes_avg)\n",
    "dist_intra_concat = get_dist_intra(classes_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist_extra_avg = get_dist_extra(classes_avg)\n",
    "dist_extra_concat = get_dist_extra(classes_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histo(dist_intra_avg, dist_extra_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "histo(dist_intra_concat, dist_extra_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = 0\n",
    "for classe, protein_list in classes_concat.items():\n",
    "    for protein_a, vec_a in protein_list.items():\n",
    "        cpt += 1\n",
    "print(cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_avg2 = np.load(\"database/next_batch/data_avg1200.npy\")[()]\n",
    "classes_concat2 = np.load(\"database/next_batch/data_concat1200.npy\")[()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del classes_concat2[\"g.37.1.2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_avg.update(classes_avg2)\n",
    "classes_concat.update(classes_concat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"database/data_avg16312.npy\", classes_avg)\n",
    "np.save(\"database/data_concat16312.npy\", classes_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes_avg[\"g.37.1.3\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
